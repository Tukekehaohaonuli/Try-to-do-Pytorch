{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2lzh_pytorch as d2l\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "(corpus_indices,char_to_idx,idx_to_char,vocab_size)=d2l.load_data_jay_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cuda\n"
     ]
    }
   ],
   "source": [
    "num_inputs,num_hiddens,num_outputs=vocab_size,256,vocab_size\n",
    "print('will use',device)\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        ts=torch.tensor(np.random.normal(0,0.01,size=shape),device=device,dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts,requires_grad=True)\n",
    "    def _three():\n",
    "        return (_one((num_inputs,num_hiddens)),\n",
    "               _one((num_hiddens,num_hiddens)),\n",
    "               torch.nn.Parameter(torch.zeros(num_hiddens,device=device,dtype=torch.float32),requires_grad=True))\n",
    "    \n",
    "    W_xi,W_hi,b_i=_three()\n",
    "    W_xf,W_hf,b_f=_three()\n",
    "    W_xo,W_ho,b_o=_three()\n",
    "    W_xc,W_hc,b_c=_three()\n",
    "    \n",
    "    W_hq=_one((num_hiddens,num_outputs))\n",
    "    b_q=torch.nn.Parameter(torch.zeros(num_outputs,device=device,dtype=torch.float32),requires_grad=True)\n",
    "    return nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm_state(batch_size,num_hiddens,device):\n",
    "    return (torch.zeros((batch_size,num_hiddens),device=device),\n",
    "           torch.zeros((batch_size,num_hiddens),device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(inputs,state,params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params\n",
    "    (H,C)=state\n",
    "    outputs=[]\n",
    "    for X in inputs:\n",
    "        I = torch.sigmoid(torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i)\n",
    "        F = torch.sigmoid(torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f)\n",
    "        O = torch.sigmoid(torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o)\n",
    "        C_tilda = torch.tanh(torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * C.tanh()\n",
    "        Y=torch.matmul(H,W_hq)+b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs,(H,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs,num_steps,batch_size,lr,clipping_theta=160,35,32,1e2,1e-2\n",
    "pred_period,pred_len,prefixes=40,50,['分开','不分开']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40,perplexity 207.835457,time 1.93 sec\n",
      " - 分开 我不不我你你 我不不我你你 我不不我你你 我不不我你你 我不不我你你 我不不我你你 我不不我你你 \n",
      " - 不分开 我不不我你你你 我不不我你你 我不不我你你 我不不我你你 我不不我你你 我不不我你你 我不不我你你\n",
      "epoch 80,perplexity 65.408312,time 2.06 sec\n",
      " - 分开 我想你你想你 我想要你不不 我不要我 我不要我 我不要我 我不要我 我不要我 我不要我 我不要我 \n",
      " - 不分开 我想你你想你 我不要你 我不要我 我不要我 我不要我 我不要我 我不要我 我不要我 我不要我 我不\n",
      "epoch 120,perplexity 14.780714,time 1.83 sec\n",
      " - 分开 我想你你的微笑 我想想这样 我想这这样活 我想要这生活 我爱你 你爱我 我想好这生活 你知后觉 你\n",
      " - 不分开 你知到我不经 我想要这样 我爱经这节活 后知后觉 你已了我 我不好好 你你怎么 你不了空 你不了空\n",
      "epoch 160,perplexity 3.910669,time 1.68 sec\n",
      " - 分开 我已那的话笑笑天的老望 我有我能爸你在一个的诗 我有在美索后后想的可原 我有你世字写 想你想不不不\n",
      " - 不分开你的画面 我想要这样 如果要发发信信命运 感谢开不口 让让一名名动动动 我以以让 你子一外在半抱 我\n"
     ]
    }
   ],
   "source": [
    "d2l.train_and_predict_rnn(lstm,get_params,init_lstm_state,num_hiddens,\n",
    "                         vocab_size,device,corpus_indices,idx_to_char,\n",
    "                         char_to_idx,False,num_epochs,num_steps,lr,\n",
    "                         clipping_theta,batch_size,pred_period,pred_len,\n",
    "                         prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40,perplexity 1.021683,time 0.30 sec\n",
      "- 分开始打呼 管家是一只会说法语举止优雅的猪 吸血前会念约翰福音做为弥补 拥有一双蓝色眼睛的凯萨琳公主 专\n",
      "- 不分开 我不再再想 我不要再想 我不 我不 我不要再想你 爱情来的太快就像龙卷风 离不开暴风圈来不及逃 我\n"
     ]
    }
   ],
   "source": [
    "lr=1e-2\n",
    "lstm_layer=nn.LSTM(input_size=vocab_size,hidden_size=num_hiddens)\n",
    "model=d2l.RNNModel(lstm_layer,vocab_size)\n",
    "d2l.train_and_predict_rnn_pytorch(model,num_hiddens,vocab_size,device,\n",
    "                                 corpus_indices,idx_to_char,char_to_idx,\n",
    "                                 num_epochs,num_steps,lr,clipping_theta,\n",
    "                                 batch_size,pred_period,pred_len,prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
